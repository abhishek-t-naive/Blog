---
layout : default
---
### 1. HAQ: Hardware-Aware Automated Quantization with Mixed Precision[[Paper Link]](https://arxiv.org/pdf/1811.08886.pdf)

- _STATED GOALS AND SOLUTIONS_

The aim of the paper is to come up with a strategy to automate the process of quantization in the neural networks and take into consideration the underlying hardware to specialize a neural network for the same. The authors have developed a tool caled HAQ to achieve the same. The tool uses Reinforcement learning to learn the best possible parameters for each of its layers (quantization widths) to achieve best performance on the underlying hardware under the latency, power etc constraints and achieving best possible accuracy.

- _SIGNIFICANT IDEAS_

THe authors shows the problem with uniform quantization of the NNs which does not take into account the variation in the redundancy in each layer of the network. Moreover, the authors showcase underlying problem when FLOPs are considered as a parameter to gauge the performance of a NN on a HW. Instead, they make use of latency and energy consumption numbers and take them in the design loop to best optimize the chosen bit widths for each layer in the network. 

- _FALLACIES AND BLIND SPOTS_

It is unclear as to how the information from the undelrying hardware (latency, energy consumption etc.) is fed to the RL agent to achieve the best possible optimization.
- _FUTURE WORK AND CONCLUSION_

With the complexity of system design increasing, I think integration with full-system simulators is necessary. Moreover, with functionalities like video captioning coming to handheld devices, there will be a need to accelerate networks other than CNN's (primarily LSTMs, LRCNs etc). Therefore, the work can be extended to incorporating the behavior of such networks on Systolic-Array based accelerators. Overall, I think it is a good step towards developing simulatio-emulation platforms for HW Accelerators design and will provide better understanding to how varying the different parameters at design time affects the final performance.

[back](/papers/papers_combine) 
